{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Local\\Temp\\ipykernel_112556\\3520666004.py:17: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, skiprows=1, header=None, delim_whitespace=True)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_frames, processed_data, scalers, avg_data_points\n\u001b[0;32m     92\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../uploads\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Directory where the .svc files are stored\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m data_frames, processed_data, scalers, avg_data_points \u001b[38;5;241m=\u001b[39m \u001b[43mupload_and_process_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_files_to_use\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of processed files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(processed_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage number of data points: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_data_points\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 66\u001b[0m, in \u001b[0;36mupload_and_process_files\u001b[1;34m(directory, num_files_to_use)\u001b[0m\n\u001b[0;32m     64\u001b[0m on_paper \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpen_status\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     65\u001b[0m in_air \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpen_status\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m \u001b[43maxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mscatter(\u001b[38;5;241m-\u001b[39mon_paper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m], on_paper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOn Paper\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m axs[i]\u001b[38;5;241m.\u001b[39mscatter(\u001b[38;5;241m-\u001b[39min_air[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m], in_air[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn Air\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m axs[i]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Data \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def upload_and_process_files(directory, num_files_to_use=None):\n",
    "    svc_files = [f for f in os.listdir(directory) if f.endswith('.svc')]\n",
    "    \n",
    "    if num_files_to_use:\n",
    "        svc_files = random.sample(svc_files, num_files_to_use)\n",
    "\n",
    "    data_frames = []\n",
    "    scalers = []\n",
    "\n",
    "    num_files = len(svc_files)\n",
    "    fig, axs = plt.subplots(1, num_files, figsize=(6*num_files, 6), constrained_layout=True)\n",
    "    if num_files == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i, filename in enumerate(svc_files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path, skiprows=1, header=None, delim_whitespace=True)\n",
    "        df.columns = ['x', 'y', 'timestamp', 'pen_status', 'pressure', 'azimuth', 'altitude']\n",
    "        \n",
    "        # Convert the 'timestamp' column to numeric for calculations\n",
    "        df['timestamp'] = pd.to_numeric(df['timestamp'])\n",
    "\n",
    "        # Sort the DataFrame by timestamp\n",
    "        df.sort_values('timestamp', inplace=True)\n",
    "\n",
    "        # Calculate the differences between consecutive timestamps\n",
    "        df['time_diff'] = df['timestamp'].diff()\n",
    "\n",
    "        # Identify the indices where the time difference is greater than 100,000 milliseconds\n",
    "        gap_indices = df.index[df['time_diff'] > 100000].tolist()\n",
    "\n",
    "        # Create an empty list to hold the new rows\n",
    "        new_rows = []\n",
    "\n",
    "        # Fill in the gaps with 70 milliseconds intervals\n",
    "        for idx in gap_indices:\n",
    "            if idx + 1 < len(df):\n",
    "                current_timestamp = df.at[idx, 'timestamp']\n",
    "                next_timestamp = df.at[idx + 1, 'timestamp']\n",
    "                num_fill_entries = (next_timestamp - current_timestamp) // 70\n",
    "\n",
    "                for i in range(1, num_fill_entries + 1):\n",
    "                    new_timestamp = current_timestamp + i * 70\n",
    "                    new_row = {\n",
    "                        'x': np.nan,\n",
    "                        'y': np.nan,\n",
    "                        'timestamp': new_timestamp,\n",
    "                        'pen_status': 0,\n",
    "                        'azimuth': df.at[idx, 'azimuth'],\n",
    "                        'altitude': df.at[idx, 'altitude'],\n",
    "                        'pressure': df.at[idx, 'pressure']\n",
    "                    }\n",
    "                    new_rows.append(new_row)\n",
    "\n",
    "        new_rows_df = pd.DataFrame(new_rows)\n",
    "        df = pd.concat([df, new_rows_df], ignore_index=True)\n",
    "        df.sort_values('timestamp', inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.drop(columns='time_diff', inplace=True)\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        data_frames.append(df)\n",
    "\n",
    "        on_paper = df[df['pen_status'] == 1]\n",
    "        in_air = df[df['pen_status'] == 0]\n",
    "        axs[i].scatter(-on_paper['y'], on_paper['x'], c='blue', s=1, alpha=0.7, label='On Paper')\n",
    "        axs[i].scatter(-in_air['y'], in_air['x'], c='red', s=1, alpha=0.7, label='In Air')\n",
    "        axs[i].set_title(f'Original Data {i + 1}')\n",
    "        axs[i].set_xlabel('-y')\n",
    "        axs[i].set_ylabel('x')\n",
    "        axs[i].legend()\n",
    "        axs[i].set_aspect('equal')\n",
    "\n",
    "        # Print the first few rows of the timestamp column\n",
    "        print(f\"Modified timestamps for file {filename}:\")\n",
    "        print(df['timestamp'].head())\n",
    "        print(\"\\n\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Ensure that scalers are created and data is processed correctly\n",
    "    if data_frames:\n",
    "        processed_data = [np.column_stack((df[['x', 'y', 'timestamp']].values, df['pen_status'].values)) \n",
    "                          for df in data_frames]\n",
    "        avg_data_points = int(np.mean([df.shape[0] for df in data_frames]))\n",
    "    else:\n",
    "        processed_data = []\n",
    "        avg_data_points = 0\n",
    "\n",
    "    return data_frames, processed_data, scalers, avg_data_points\n",
    "\n",
    "directory = '../uploads'  # Directory where the .svc files are stored\n",
    "data_frames, processed_data, scalers, avg_data_points = upload_and_process_files(directory, num_files_to_use=1)\n",
    "print(f\"Number of processed files: {len(processed_data)}\")\n",
    "print(f\"Average number of data points: {avg_data_points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'azimuth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'azimuth'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 40\u001b[0m\n\u001b[0;32m     32\u001b[0m new_timestamp \u001b[38;5;241m=\u001b[39m current_timestamp \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Create a new row to fill in with NaN for x and y\u001b[39;00m\n\u001b[0;32m     35\u001b[0m new_row \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,  \u001b[38;5;66;03m# Set x to NaN\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,  \u001b[38;5;66;03m# Set y to NaN\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: new_timestamp,\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpen_status\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,        \u001b[38;5;66;03m# You can set this to your desired value\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mazimuth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mazimuth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,   \u001b[38;5;66;03m# Use the current azimuth value\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maltitude\u001b[39m\u001b[38;5;124m'\u001b[39m: df\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maltitude\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;66;03m# Use the current altitude value\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpressure\u001b[39m\u001b[38;5;124m'\u001b[39m: df\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpressure\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Use the current pressure value\u001b[39;00m\n\u001b[0;32m     43\u001b[0m }\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Append the new row to the list of new rows\u001b[39;00m\n\u001b[0;32m     46\u001b[0m new_rows\u001b[38;5;241m.\u001b[39mappend(new_row)\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:2575\u001b[0m, in \u001b[0;36m_AtIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2572\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mloc[key]\n\u001b[1;32m-> 2575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:2527\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2526\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_key(key)\n\u001b[1;32m-> 2527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4214\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   4211\u001b[0m     series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ixs(col, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   4212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[index]\n\u001b[1;32m-> 4214\u001b[0m series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_item_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4215\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[0;32m   4217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   4218\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   4219\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   4220\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4638\u001b[0m, in \u001b[0;36mDataFrame._get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   4633\u001b[0m res \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(item)\n\u001b[0;32m   4634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4635\u001b[0m     \u001b[38;5;66;03m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;66;03m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[1;32m-> 4638\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4639\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ixs(loc, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   4641\u001b[0m     cache[item] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'azimuth'"
     ]
    }
   ],
   "source": [
    "# Assume the first processed DataFrame is the one we want to read\n",
    "df = data_frames[0]  # Using the first DataFrame from the list returned by `upload_and_process_files`\n",
    "\n",
    "# Convert the 'timestamp' column to numeric for calculations (if not already done)\n",
    "df['timestamp'] = pd.to_numeric(df['timestamp'])\n",
    "\n",
    "# Sort the DataFrame by timestamp (should already be sorted in the function)\n",
    "df.sort_values('timestamp', inplace=True)\n",
    "\n",
    "# Calculate the differences between consecutive timestamps (optional for gap finding)\n",
    "df['time_diff'] = df['timestamp'].diff()\n",
    "\n",
    "# Identify the indices where the time difference is greater than 30,000 milliseconds\n",
    "gap_indices = df.index[df['time_diff'] > 30000].tolist()\n",
    "\n",
    "# Create an empty list to hold the new rows\n",
    "new_rows = []\n",
    "\n",
    "# Fill in the gaps with 70 milliseconds intervals\n",
    "for idx in gap_indices:\n",
    "    # Check if the next index is valid\n",
    "    if idx + 1 < len(df):\n",
    "        # Get the current and next timestamps\n",
    "        current_timestamp = df.at[idx, 'timestamp']\n",
    "        next_timestamp = df.at[idx + 1, 'timestamp']\n",
    "        \n",
    "        # Calculate how many entries we need to fill in\n",
    "        num_fill_entries = (next_timestamp - current_timestamp) // 20000\n",
    "\n",
    "        # Generate the timestamps to fill the gap\n",
    "        for i in range(1, num_fill_entries + 1):\n",
    "            new_timestamp = current_timestamp + i * 70\n",
    "            \n",
    "            # Create a new row to fill in with NaN for x and y\n",
    "            new_row = {\n",
    "                'x': np.nan,  # Set x to NaN\n",
    "                'y': np.nan,  # Set y to NaN\n",
    "                'timestamp': new_timestamp,\n",
    "                'pen_status': 0,        # You can set this to your desired value\n",
    "                'azimuth': df.at[idx, 'azimuth'],   # Use the current azimuth value\n",
    "                'altitude': df.at[idx, 'altitude'], # Use the current altitude value\n",
    "                'pressure': df.at[idx, 'pressure']  # Use the current pressure value\n",
    "            }\n",
    "            \n",
    "            # Append the new row to the list of new rows\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "# Create a DataFrame from the new rows\n",
    "new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Concatenate the original DataFrame with the new rows DataFrame\n",
    "df = pd.concat([df, new_rows_df], ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame by timestamp to maintain order\n",
    "df.sort_values('timestamp', inplace=True)\n",
    "\n",
    "# Reset index after sorting\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Interpolate NaN values in 'x' and 'y' columns based on nearest neighbors\n",
    "# Check for NaN entries before interpolation\n",
    "if df[['x', 'y']].isnull().any().any():\n",
    "    df[['x', 'y']] = df[['x', 'y']].interpolate(method='linear')\n",
    "\n",
    "# Remove any remaining rows with NaN values in 'x' or 'y'\n",
    "df.dropna(subset=['x', 'y'], inplace=True)\n",
    "\n",
    "# Drop the 'time_diff' column as it's no longer needed\n",
    "df.drop(columns=['time_diff'], inplace=True)\n",
    "\n",
    "# Plotting the result\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the rest of the data\n",
    "on_paper = df[df['pen_status'] == 1]\n",
    "in_air = df[df['pen_status'] == 0]\n",
    "plt.scatter(-on_paper['y'], on_paper['x'], c='blue', s=1, alpha=0.7, label='On Paper')\n",
    "plt.scatter(-in_air['y'], in_air['x'], c='red', s=1, alpha=0.7, label='In Air')\n",
    "\n",
    "# Labels and titles\n",
    "plt.title('Processed Data with Interpolated Points')\n",
    "plt.xlabel('-y')\n",
    "plt.ylabel('x')\n",
    "plt.legend()\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Now apply the filtering step before processing with scalers\n",
    "# Process and scale the data\n",
    "processed_data = [np.column_stack((scaler.transform(df[['x', 'y', 'timestamp']]), df['pen_status'].values)) \n",
    "                  for df, scaler in zip(data_frames, scalers)]\n",
    "\n",
    "avg_data_points = int(np.mean([df.shape[0] for df in data_frames]))\n",
    "print(f\"Number of processed files: {len(processed_data)}\")\n",
    "print(f\"Average number of data points: {avg_data_points}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 (modified)\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, beta=1.0):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(4,)),  # Changed to 4 for x, y, timestamp, pen_status\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(latent_dim * 2)\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(4)  # Changed to 4 for x, y, timestamp, pen_status\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z):\n",
    "        decoded = self.decoder(z)\n",
    "        xy_timestamp = tf.sigmoid(decoded[:, :3])  # x, y, and timestamp\n",
    "        pen_status = tf.sigmoid(decoded[:, 3])\n",
    "        return tf.concat([xy_timestamp, tf.expand_dims(pen_status, -1)], axis=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, logvar = self.encode(inputs)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decode(z), mean, logvar\n",
    "\n",
    "\n",
    "# New: LSTM Discriminator for GAN\n",
    "class LSTMDiscriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LSTMDiscriminator, self).__init__()\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(64, return_sequences=True, input_shape=(None, 4)))  # LSTM for sequence learning\n",
    "        self.model.add(LSTM(32))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Function to compute VAE loss\n",
    "def compute_loss(model, x):\n",
    "    x_reconstructed, mean, logvar = model(x)\n",
    "    reconstruction_loss_xy_timestamp = tf.reduce_mean(tf.keras.losses.mse(x[:, :3], x_reconstructed[:, :3]))\n",
    "    reconstruction_loss_pen = tf.reduce_mean(tf.keras.losses.binary_crossentropy(x[:, 3], x_reconstructed[:, 3]))\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
    "    return reconstruction_loss_xy_timestamp + reconstruction_loss_pen, kl_loss, model.beta * kl_loss\n",
    "\n",
    "\n",
    "latent_dim = 64\n",
    "beta = 0.000001\n",
    "learning_rate = 0.001\n",
    "\n",
    "vae = VAE(latent_dim, beta)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 (modified)\n",
    "def generate_augmented_data(model, num_augmented_files, avg_data_points, processed_data, variability=0.05, latent_variability=1.0):\n",
    "    augmented_datasets = []\n",
    "    num_input_files = len(processed_data)\n",
    "    \n",
    "    for i in range(num_augmented_files):\n",
    "        # Select input data sequentially, cycling through if necessary\n",
    "        selected_data = processed_data[i % num_input_files]\n",
    "        \n",
    "        mean, logvar = model.encode(tf.convert_to_tensor(selected_data, dtype=tf.float32))\n",
    "        \n",
    "        # Increase variability in the latent space\n",
    "        z = model.reparameterize(mean, logvar * latent_variability)\n",
    "        \n",
    "        augmented_data = model.decode(z).numpy()\n",
    "\n",
    "        # Post-process pen status\n",
    "        augmented_data[:, 3] = post_process_pen_status(augmented_data[:, 3])\n",
    "        \n",
    "        # Ensure timestamps are in sequence\n",
    "        augmented_data[:, 2] = np.sort(augmented_data[:, 2])\n",
    "        \n",
    "        augmented_datasets.append(augmented_data)\n",
    "\n",
    "    return augmented_datasets\n",
    "\n",
    "# The post_process_pen_status function remains unchanged\n",
    "def post_process_pen_status(pen_status, threshold=0.5, min_segment_length=5):\n",
    "    binary_pen_status = (pen_status > threshold).astype(int)\n",
    "    \n",
    "    # Smooth out rapid changes\n",
    "    for i in range(len(binary_pen_status) - min_segment_length):\n",
    "        if np.all(binary_pen_status[i:i+min_segment_length] == binary_pen_status[i]):\n",
    "            binary_pen_status[i:i+min_segment_length] = binary_pen_status[i]\n",
    "    \n",
    "    return binary_pen_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "Epoch 1/200: 100%|██████████| 529/529 [00:09<00:00, 58.56batch/s, Generator Loss=nan, Reconstruction Loss=nan, KL Loss=nan]       \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 91\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m processed_data:\n\u001b[0;32m     90\u001b[0m     augmented_data \u001b[38;5;241m=\u001b[39m vae\u001b[38;5;241m.\u001b[39mdecode(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(shape\u001b[38;5;241m=\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], latent_dim)))\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 91\u001b[0m     rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmented_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     92\u001b[0m     nrmse \u001b[38;5;241m=\u001b[39m rmse \u001b[38;5;241m/\u001b[39m (data[:, :\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m data[:, :\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mmin())\n\u001b[0;32m     93\u001b[0m     nrmse_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m nrmse\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    504\u001b[0m         )\n\u001b[1;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:112\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m    109\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m--> 112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# Cell 5\n",
    "@tf.function\n",
    "def train_vae_step(model, x, optimizer, lstm_discriminator=None):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_reconstructed, mean, logvar = model(x)\n",
    "        reconstruction_loss, kl_loss, total_kl_loss = compute_loss(model, x)\n",
    "        \n",
    "        # Add LSTM discriminator loss if available\n",
    "        if lstm_discriminator is not None:\n",
    "            real_predictions = lstm_discriminator(tf.expand_dims(x, axis=0))\n",
    "            fake_predictions = lstm_discriminator(tf.expand_dims(x_reconstructed, axis=0))\n",
    "            discriminator_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(real_predictions), real_predictions) +\n",
    "                                                tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_predictions), fake_predictions))\n",
    "            generator_loss = reconstruction_loss + total_kl_loss - 0.1 * discriminator_loss  # Adjust the weight as needed\n",
    "        else:\n",
    "            generator_loss = reconstruction_loss + total_kl_loss\n",
    "    \n",
    "    gradients = tape.gradient(generator_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return generator_loss, reconstruction_loss, kl_loss\n",
    "\n",
    "@tf.function\n",
    "def train_lstm_step(lstm_model, real_data, generated_data, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        real_predictions = lstm_model(real_data)\n",
    "        generated_predictions = lstm_model(generated_data)\n",
    "        real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_predictions), real_predictions)\n",
    "        generated_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(generated_predictions), generated_predictions)\n",
    "        total_loss = real_loss + generated_loss\n",
    "    gradients = tape.gradient(total_loss, lstm_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, lstm_model.trainable_variables))\n",
    "    return total_loss\n",
    "\n",
    "# Initialize LSTM discriminator and optimizer\n",
    "lstm_discriminator = LSTMDiscriminator()\n",
    "lstm_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "batch_size = 64\n",
    "train_datasets = [tf.data.Dataset.from_tensor_slices(data).shuffle(10000).batch(batch_size) for data in processed_data]\n",
    "\n",
    "# Set up alternating epochs\n",
    "vae_epochs = 19\n",
    "lstm_interval = 10\n",
    "epochs = 200\n",
    "\n",
    "generator_loss_history = []\n",
    "reconstruction_loss_history = []\n",
    "kl_loss_history = []\n",
    "nrmse_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    generator_loss = 0 \n",
    "    reconstruction_loss_sum = 0\n",
    "    kl_loss_sum = 0\n",
    "    num_batches = sum(len(dataset) for dataset in train_datasets)\n",
    "\n",
    "    with tqdm(total=num_batches, desc=f'Epoch {epoch+1}/{epochs}', unit='batch') as pbar:\n",
    "        for dataset in train_datasets:\n",
    "            for batch in dataset:\n",
    "                # Use LSTM discriminator after vae_epochs and every lstm_interval\n",
    "                use_lstm = epoch >= vae_epochs and (epoch - vae_epochs) % lstm_interval == 0\n",
    "                generator_loss_batch, reconstruction_loss, kl_loss = train_vae_step(vae, batch, optimizer, lstm_discriminator if use_lstm else None)\n",
    "                generator_loss += generator_loss_batch\n",
    "                reconstruction_loss_sum += reconstruction_loss\n",
    "                kl_loss_sum += kl_loss\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'Generator Loss': float(generator_loss_batch), 'Reconstruction Loss': float(reconstruction_loss), 'KL Loss': float(kl_loss)})\n",
    "\n",
    "    # Train the LSTM discriminator every lstm_interval epochs after vae_epochs\n",
    "    if epoch >= vae_epochs and (epoch - vae_epochs) % lstm_interval == 0:\n",
    "        for data in processed_data:\n",
    "            augmented_data = vae.decode(tf.random.normal(shape=(data.shape[0], latent_dim))).numpy()\n",
    "            real_data = tf.expand_dims(data, axis=0)  # Reshape for LSTM input\n",
    "            generated_data = tf.expand_dims(augmented_data, axis=0)\n",
    "\n",
    "            lstm_loss = train_lstm_step(lstm_discriminator, real_data, generated_data, lstm_optimizer)\n",
    "        print(f'LSTM training at epoch {epoch+1}: Discriminator Loss = {lstm_loss.numpy()}')\n",
    "\n",
    "    avg_generator_loss = generator_loss / num_batches  # Update the average calculation\n",
    "    avg_reconstruction_loss = reconstruction_loss_sum / num_batches\n",
    "    avg_kl_loss = kl_loss_sum / num_batches\n",
    "\n",
    "    generator_loss_history.append(avg_generator_loss)  # Update history list\n",
    "    reconstruction_loss_history.append(avg_reconstruction_loss)\n",
    "    kl_loss_history.append(avg_kl_loss)\n",
    "\n",
    "    # Calculate NRMSE\n",
    "    nrmse_sum = 0\n",
    "    for data in processed_data:\n",
    "        augmented_data = vae.decode(tf.random.normal(shape=(data.shape[0], latent_dim))).numpy()\n",
    "        rmse = np.sqrt(mean_squared_error(data[:, :2], augmented_data[:, :2]))\n",
    "        nrmse = rmse / (data[:, :2].max() - data[:, :2].min())\n",
    "        nrmse_sum += nrmse\n",
    "    \n",
    "    nrmse_avg = nrmse_sum / len(processed_data)\n",
    "\n",
    "    nrmse_history.append(nrmse_avg)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Generator Loss = {avg_generator_loss:.6f}, Reconstruction Loss = {avg_reconstruction_loss:.6f}, KL Divergence Loss = {avg_kl_loss:.6f}\")\n",
    "    print(f\"NRMSE = {nrmse_avg:.6f}\")\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        num_augmented_files = 1  # You can change this number as needed\n",
    "        augmented_datasets = generate_augmented_data(vae, num_augmented_files, avg_data_points, processed_data, latent_variability=100.0)\n",
    "\n",
    "        # Visualize the original and augmented data side by side\n",
    "        fig, axs = plt.subplots(1, num_augmented_files + len(processed_data), figsize=(6*(num_augmented_files + len(processed_data)), 6))\n",
    "        \n",
    "        for i, original_data in enumerate(processed_data):\n",
    "            original_on_paper = original_data[original_data[:, 3] == 1]\n",
    "            original_in_air = original_data[original_data[:, 3] == 0]\n",
    "            axs[i].scatter(original_on_paper[:, 0], original_on_paper[:, 1], c='b', s=1, label='On Paper')\n",
    "            axs[i].scatter(original_in_air[:, 0], original_in_air[:, 1], c='r', s=1, label='In Air')\n",
    "            axs[i].set_title(f'Original Data {i+1}')\n",
    "            axs[i].set_xlabel('x')\n",
    "            axs[i].set_ylabel('y')\n",
    "            axs[i].set_aspect('equal')\n",
    "            axs[i].legend()\n",
    "\n",
    "        # Set consistent axis limits for square aspect ratio\n",
    "        x_min = min(data[:, 0].min() for data in processed_data)\n",
    "        x_max = max(data[:, 0].max() for data in processed_data)\n",
    "        y_min = min(data[:, 1].min() for data in processed_data)\n",
    "        y_max = max(data[:, 1].max() for data in processed_data)\n",
    "\n",
    "        for i, augmented_data in enumerate(augmented_datasets):\n",
    "            augmented_on_paper = augmented_data[augmented_data[:, 3] == 1]\n",
    "            augmented_in_air = augmented_data[augmented_data[:, 3] == 0]\n",
    "            axs[i+len(processed_data)].scatter(augmented_on_paper[:, 0], augmented_on_paper[:, 1], c='b', s=1, label='On Paper')\n",
    "            axs[i+len(processed_data)].scatter(augmented_in_air[:, 0], augmented_in_air[:, 1], c='r', s=1, label='In Air')\n",
    "            axs[i+len(processed_data)].set_title(f'Augmented Data {i+1}')\n",
    "            axs[i+len(processed_data)].set_xlabel('x')\n",
    "            axs[i+len(processed_data)].set_ylabel('y')\n",
    "            axs[i+len(processed_data)].set_aspect('equal')\n",
    "            axs[i+len(processed_data)].set_xlim(x_min, x_max)\n",
    "            axs[i+len(processed_data)].set_ylim(y_min, y_max)\n",
    "            axs[i+len(processed_data)].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Final output of metrics\n",
    "print(f\"Final NRMSE: {nrmse_history[-1]:.6f}\")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "plt.show()\n",
    "\n",
    "# Plot generator loss history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(generator_loss_history, label='Generator Loss')  # Update label\n",
    "plt.plot(reconstruction_loss_history, label='Reconstruction Loss')\n",
    "plt.plot(kl_loss_history, label='KL Divergence Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot NRMSE history\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(nrmse_history, label='NRMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('NRMSE')\n",
    "plt.title('Normalized Root Mean Squared Error Over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for predictive score\n",
    "def prepare_predictive_data(combined_data, augmented_datasets):\n",
    "    # Split real data into features (all except last time step) and target (last time step)\n",
    "    X_real = combined_data[:, :-1]  # All except last time step\n",
    "    y_real = combined_data[:, -1]   # Last time step\n",
    "\n",
    "    # Split synthetic data into features and target\n",
    "    X_synthetic = np.vstack([data[:, :-1] for data in augmented_datasets])  # All except last time step\n",
    "    y_synthetic = np.hstack([data[:, -1] for data in augmented_datasets])   # Last time step\n",
    "\n",
    "    return X_real, y_real, X_synthetic, y_synthetic\n",
    "\n",
    "# Function to calculate Post-Hoc Predictive Score with MAPE\n",
    "def post_hoc_predictive_score(combined_data, augmented_datasets):\n",
    "    # Prepare the real and synthetic data for training and testing\n",
    "    X_real, y_real, X_synthetic, y_synthetic = prepare_predictive_data(combined_data, augmented_datasets)\n",
    "\n",
    "    # Normalize the feature data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_real_scaled = scaler.fit_transform(X_real)\n",
    "    X_synthetic_scaled = scaler.transform(X_synthetic)\n",
    "\n",
    "    mapes = []  # List to store MAPE values\n",
    "\n",
    "    # Train and evaluate the model multiple times for stability\n",
    "    for _ in range(10):\n",
    "        model = Sequential()  # Initialize the model\n",
    "        model.add(Dense(50, input_dim=X_synthetic_scaled.shape[1], activation='relu'))  # Add a Dense layer\n",
    "        model.add(Dense(1))  # Output layer\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        # Train the model on synthetic data\n",
    "        model.fit(X_synthetic_scaled, y_synthetic, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "        # Predict on real data\n",
    "        y_pred = model.predict(X_real_scaled)\n",
    "        # Calculate MAPE for the current iteration\n",
    "        mape = mean_absolute_percentage_error(y_real, y_pred)\n",
    "        mapes.append(mape)  # Append MAPE to the list\n",
    "\n",
    "    # Calculate mean and standard deviation of MAPE values\n",
    "    mean_mape = np.mean(mapes)\n",
    "    std_mape = np.std(mapes)\n",
    "    return mean_mape, std_mape\n",
    "\n",
    "# Example usage (assuming combined_data and augmented_datasets are defined)\n",
    "mean_mape, std_mape = post_hoc_predictive_score(combined_data, augmented_datasets)\n",
    "# Print the calculated mean and standard deviation of MAPE\n",
    "print(f\"Mean MAPE: {mean_mape}, Std MAPE: {std_mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for LSTM\n",
    "def prepare_lstm_data(combined_data, augmented_datasets):\n",
    "    # Print the shape of the combined data and the number of augmented datasets\n",
    "    print(f\"Combined Data Shape: {combined_data.shape}\")\n",
    "    print(f\"Augmented Datasets Length: {len(augmented_datasets)}\")\n",
    "\n",
    "    # Stack combined data and augmented datasets into a single array\n",
    "    X = np.vstack([combined_data] + augmented_datasets)\n",
    "\n",
    "    # Create labels: 0 for combined data, 1 for each augmented dataset\n",
    "    y = np.hstack([np.zeros(len(combined_data))] + [np.ones(data.shape[0]) for data in augmented_datasets])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Function to calculate Post-Hoc Discriminative Score with LSTM\n",
    "def post_hoc_discriminative_score(combined_data, augmented_datasets):\n",
    "    # Prepare the data for LSTM\n",
    "    X, y = prepare_lstm_data(combined_data, augmented_datasets)\n",
    "\n",
    "    # Initialize K-Fold cross-validation with 10 splits\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    accuracies = []  # List to store accuracy scores for each fold\n",
    "\n",
    "    # Loop through each fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Reshape the data for LSTM input: (samples, time steps, features)\n",
    "        X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "        X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "        # Build the LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))  # LSTM layer\n",
    "        model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_classes = np.where(y_pred > 0.5, 1, 0)  # Threshold predictions at 0.5 for binary classification\n",
    "\n",
    "        # Calculate the accuracy for the current fold\n",
    "        accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "        accuracies.append(accuracy)  # Append the accuracy to the list\n",
    "\n",
    "    # Calculate the mean and standard deviation of the accuracy scores\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Example usage (assuming combined_data and augmented_datasets are defined)\n",
    "mean_acc, std_acc = post_hoc_discriminative_score(combined_data, augmented_datasets)\n",
    "# Print the calculated mean and standard deviation of accuracy\n",
    "print(f\"Mean Accuracy: {mean_acc}, Std Accuracy: {std_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "def download_augmented_data(augmented_datasets, scaler, directory='augmented_data'):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for i, augmented_data in enumerate(augmented_datasets):\n",
    "        augmented_data_original_scale = np.column_stack((\n",
    "            scaler.inverse_transform(augmented_data[:, :3]),  # Inverse scale x, y, timestamp\n",
    "            augmented_data[:, 3]  # Keep pen_status unchanged\n",
    "        ))\n",
    "\n",
    "        # Save each augmented dataset to a CSV file\n",
    "        filename = os.path.join(directory, f'augmented_data_{i+1}.csv')\n",
    "        pd.DataFrame(augmented_data_original_scale, columns=['x', 'y', 'timestamp', 'pen_status']).to_csv(filename, index=False)\n",
    "        print(f\"Saved {filename}\")\n",
    "\n",
    "# Call the updated function with the same scaler for all augmented datasets\n",
    "download_augmented_data(augmented_datasets, scalers[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7 Function to visualize the latent space with a color gradient\n",
    "def visualize_latent_space(model, data, perplexity=5, learning_rate=200, n_iter=250):\n",
    "    # Encode data into the latent space\n",
    "    latent_means, _ = model.encode(tf.convert_to_tensor(data, dtype=tf.float32))\n",
    "    latent_means_np = latent_means.numpy()\n",
    "    \n",
    "    # Use t-SNE to reduce dimensionality to 2D\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter)\n",
    "    latent_2d = tsne.fit_transform(latent_means_np)\n",
    "    \n",
    "    # Create a color map for the latent points\n",
    "    norm = plt.Normalize(vmin=np.min(latent_means_np), vmax=np.max(latent_means_np))\n",
    "    cmap = plt.cm.cividis  # You can change the colormap to 'plasma', 'inferno', etc.\n",
    "    colors = cmap(norm(latent_means_np).sum(axis=1))  # Coloring based on the sum of latent variables\n",
    "    \n",
    "    # Plot the 2D t-SNE result with the color map\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=colors, s=5, alpha=0.6)\n",
    "    plt.colorbar(scatter)  # Add a color bar for the gradient\n",
    "    plt.title('Latent Space Visualization using t-SNE')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize latent space for one of the processed datasets\n",
    "visualize_latent_space(vae, processed_data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
