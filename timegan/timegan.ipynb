{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 1\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename='process_svc_folder.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 2\n",
    "# Define TimeGAN function\n",
    "def timegan(ori_data, parameters):\n",
    "    \"\"\"TimeGAN for generating synthetic time-series data.\"\"\"\n",
    "    # Disable eager execution for compatibility with tf.compat.v1\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "    # Basic Parameters\n",
    "    no, seq_len, dim = ori_data.shape\n",
    "\n",
    "    # Reverse scaling\n",
    "    def reverse_minmax_scaling(data, min_val, max_val):\n",
    "        return data * (max_val + 1e-7) + min_val\n",
    "    \n",
    "    # Define Min-Max Normalizer\n",
    "    def MinMaxScaler(data):\n",
    "        min_val = np.min(np.min(data, axis=0), axis=0)\n",
    "        data = data - min_val\n",
    "        max_val = np.max(np.max(data, axis=0), axis=0)\n",
    "        norm_data = data / (max_val + 1e-7)\n",
    "        return norm_data, min_val, max_val\n",
    "\n",
    "    # Normalize data\n",
    "    ori_data, min_val, max_val = MinMaxScaler(ori_data)\n",
    "\n",
    "    # Network Parameters\n",
    "    hidden_dim = parameters['hidden_dim']\n",
    "    num_layers = parameters['num_layers']\n",
    "    iterations = parameters['iterations']\n",
    "    batch_size = parameters['batch_size']\n",
    "    module_name = parameters['module']\n",
    "    gamma = 1\n",
    "    z_dim = dim\n",
    "    # Input placeholders\n",
    "    X = tf.compat.v1.placeholder(tf.float32, [None, None, dim], name=\"myinput_x\")\n",
    "    Z = tf.compat.v1.placeholder(tf.float32, [None, None, dim], name=\"myinput_z\")\n",
    "    T = tf.compat.v1.placeholder(tf.int32, [None], name=\"myinput_t\")\n",
    "\n",
    "    # Define RNN cell type\n",
    "    def rnn_cell(module_name, hidden_dim):\n",
    "        if module_name == 'gru':\n",
    "            return tf.keras.layers.GRUCell(hidden_dim)\n",
    "        elif module_name == 'lstm':\n",
    "            return tf.keras.layers.LSTMCell(hidden_dim)\n",
    "        else:\n",
    "            return tf.keras.layers.SimpleRNNCell(hidden_dim)\n",
    "\n",
    "    # Embedder\n",
    "    def embedder(X, T):\n",
    "        with tf.compat.v1.variable_scope(\"embedder\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            cells = [tf.keras.layers.GRUCell(hidden_dim) for _ in range(num_layers)]\n",
    "            rnn = tf.keras.layers.RNN(cells, return_sequences=True)\n",
    "            H = rnn(X)\n",
    "        return H\n",
    "\n",
    "    # Recovery\n",
    "    def recovery(H, T):\n",
    "        with tf.compat.v1.variable_scope(\"recovery\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            cells = [tf.keras.layers.GRUCell(hidden_dim) for _ in range(num_layers)]\n",
    "            rnn = tf.keras.layers.RNN(cells, return_sequences=True)\n",
    "            H_output = rnn(H)\n",
    "            X_tilde = tf.keras.layers.Dense(dim)(H_output)  # Ensure output matches input feature dimension\n",
    "        return X_tilde\n",
    "\n",
    "    # Generator\n",
    "    def generator(Z, T):\n",
    "        with tf.compat.v1.variable_scope(\"generator\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            cells = [rnn_cell(module_name, hidden_dim) for _ in range(num_layers)]\n",
    "            g_cell = tf.keras.layers.RNN(cells, return_sequences=True)\n",
    "            g_outputs = g_cell(Z)\n",
    "            E = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.sigmoid)(g_outputs)\n",
    "        return E\n",
    "\n",
    "    # Supervisor\n",
    "    def supervisor(H, T):\n",
    "        with tf.compat.v1.variable_scope(\"supervisor\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            cells = [rnn_cell(module_name, hidden_dim) for _ in range(num_layers - 1)]\n",
    "            s_cell = tf.keras.layers.RNN(cells, return_sequences=True)\n",
    "            s_outputs = s_cell(H)\n",
    "            S = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.sigmoid)(s_outputs)\n",
    "        return S\n",
    "\n",
    "    # Discriminator\n",
    "    def discriminator(H, T):\n",
    "        with tf.compat.v1.variable_scope(\"discriminator\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            cells = [rnn_cell(module_name, hidden_dim) for _ in range(num_layers)]\n",
    "            d_cell = tf.keras.layers.RNN(cells, return_sequences=True)\n",
    "            d_outputs = d_cell(H)\n",
    "            Y_hat = tf.keras.layers.Dense(1, activation=None)(d_outputs)\n",
    "        return Y_hat\n",
    "\n",
    "    # Build the TimeGAN model\n",
    "    H = embedder(X, T)\n",
    "    X_tilde = recovery(H, T)\n",
    "    E_hat = generator(Z, T)\n",
    "    H_hat_supervise = supervisor(H, T)\n",
    "    H_hat = E_hat + H_hat_supervise\n",
    "    X_hat = recovery(H_hat, T)\n",
    "    Y_fake = discriminator(H_hat, T)\n",
    "    Y_real = discriminator(H, T)\n",
    "    Y_fake_e = discriminator(E_hat, T)\n",
    "\n",
    "    # Loss functions\n",
    "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Y_real, labels=tf.ones_like(Y_real)))\n",
    "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Y_fake, labels=tf.zeros_like(Y_fake)))\n",
    "    d_loss_fake_e = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Y_fake_e, labels=tf.zeros_like(Y_fake_e)))\n",
    "    d_loss = d_loss_real + d_loss_fake + gamma * d_loss_fake_e\n",
    "\n",
    "    g_loss_u = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Y_fake, labels=tf.ones_like(Y_fake)))\n",
    "    g_loss_s = tf.reduce_mean(tf.keras.losses.MeanSquaredError()(H_hat_supervise[:, :-1], H[:, 1:]))\n",
    "    g_loss_v = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Y_fake_e, labels=tf.ones_like(Y_fake_e)))\n",
    "    g_loss = g_loss_u + gamma * g_loss_s + 0.1 * g_loss_v\n",
    "\n",
    "    e_loss = tf.reduce_mean(tf.keras.losses.MeanSquaredError()(X, X_tilde))\n",
    "\n",
    "    # Optimizers using tf.keras.optimizers.Adam\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "\n",
    "    # Training Loop\n",
    "    train_embedder = optimizer.minimize(e_loss)\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        for it in range(iterations):\n",
    "            Z_mb = np.random.uniform(0, 1, [no, seq_len, dim])\n",
    "            T_mb = [seq_len] * no\n",
    "\n",
    "            # Train embedder\n",
    "            _, e_loss_val = sess.run([train_embedder, e_loss], feed_dict={X: ori_data, Z: Z_mb, T: T_mb})\n",
    "\n",
    "\n",
    "            print(f\"Iteration {it}: Embedder loss = {e_loss_val}\")\n",
    "\n",
    "        # Generate synthetic data\n",
    "        synthetic_data = sess.run(X_tilde, feed_dict={X: ori_data, T: T_mb})\n",
    "\n",
    "    # Reverse normalization and round to integer format\n",
    "    synthetic_data = reverse_minmax_scaling(synthetic_data, min_val, max_val)\n",
    "    synthetic_data = np.round(synthetic_data).astype(int)\n",
    "\n",
    "    return synthetic_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3\n",
    "\n",
    "# Function to load and process SVC files\n",
    "def load_svc(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    seq_len = int(lines[0].strip())\n",
    "    \n",
    "    if seq_len <= 0:\n",
    "        raise ValueError(f\"Invalid sequence length {seq_len} in file {file_path}\")\n",
    "    \n",
    "    data = np.array([list(map(float, line.split())) for line in lines[1:]])\n",
    "    \n",
    "    # Ensure the data length matches the expected sequence length\n",
    "    if data.shape[0] % seq_len != 0:\n",
    "        raise ValueError(f\"Data length {data.shape[0]} is not a multiple of sequence length {seq_len}\")\n",
    "    \n",
    "    dim = data.shape[1]\n",
    "    return data.reshape(-1, seq_len, dim)  # Reshape to (batch, seq_len, dim)\n",
    "\n",
    "\n",
    "def process_single_file(file_name, input_folder, output_folder, parameters):\n",
    "    \"\"\"Process a single SVC file with error handling.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        \n",
    "        logging.info(f\"Starting processing of file: {file_name}\")\n",
    "        \n",
    "        # Load and process the data\n",
    "        ori_data = load_svc(file_path)\n",
    "        \n",
    "        # Generate synthetic data\n",
    "        synthetic_data = timegan(ori_data, parameters)\n",
    "        \n",
    "        # Save synthetic data\n",
    "        output_path = os.path.join(output_folder, f\"synthetic_{file_name}\")\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(f\"{synthetic_data.shape[1]}\\n\")\n",
    "            for row in synthetic_data.reshape(-1, synthetic_data.shape[-1]):\n",
    "                f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        logging.info(f\"Completed processing file {file_name}. Time taken: {processing_time:.2f} seconds\")\n",
    "        \n",
    "        return True, file_name, processing_time\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {file_name}: {str(e)}\")\n",
    "        return False, file_name, str(e)\n",
    "\n",
    "def process_svc_folder_parallel(input_folder, output_folder, parameters, num_processes=None):\n",
    "    \"\"\"Process multiple SVC files in parallel.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Get list of SVC files\n",
    "    svc_files = [f for f in os.listdir(input_folder) if f.endswith('.svc')]\n",
    "    \n",
    "    if not svc_files:\n",
    "        logging.warning(\"No SVC files found in the input folder\")\n",
    "        return\n",
    "    \n",
    "    # Determine number of processes to use\n",
    "    if num_processes is None:\n",
    "        num_processes = min(mp.cpu_count(), len(svc_files))\n",
    "    \n",
    "    logging.info(f\"Starting parallel processing with {num_processes} processes\")\n",
    "    \n",
    "    # Create partial function with fixed arguments\n",
    "    process_func = partial(\n",
    "        process_single_file,\n",
    "        input_folder=input_folder,\n",
    "        output_folder=output_folder,\n",
    "        parameters=parameters\n",
    "    )\n",
    "    \n",
    "    # Initialize the pool and process files\n",
    "    try:\n",
    "        with mp.Pool(processes=num_processes) as pool:\n",
    "            results = pool.map(process_func, svc_files)\n",
    "        \n",
    "        # Process results\n",
    "        successful_files = []\n",
    "        failed_files = []\n",
    "        total_processing_time = 0\n",
    "        \n",
    "        for success, file_name, result in results:\n",
    "            if success:\n",
    "                successful_files.append(file_name)\n",
    "                total_processing_time += result\n",
    "            else:\n",
    "                failed_files.append((file_name, result))\n",
    "        \n",
    "        # Log summary\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        logging.info(f\"\\nProcessing Summary:\")\n",
    "        logging.info(f\"Total files processed: {len(svc_files)}\")\n",
    "        logging.info(f\"Successfully processed: {len(successful_files)}\")\n",
    "        logging.info(f\"Failed to process: {len(failed_files)}\")\n",
    "        logging.info(f\"Total wall time: {total_time:.2f} seconds\")\n",
    "        logging.info(f\"Total processing time: {total_processing_time:.2f} seconds\")\n",
    "        \n",
    "        if failed_files:\n",
    "            logging.error(\"\\nFailed files:\")\n",
    "            for file_name, error in failed_files:\n",
    "                logging.error(f\"{file_name}: {error}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in parallel processing: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Starting parallel processing with 2 processes'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cell 4\n",
    "# Parameters\n",
    "timegan_params = {\n",
    "    'hidden_dim': 24,\n",
    "    'num_layers': 3,\n",
    "    'iterations': 100,\n",
    "    'batch_size': 1024,\n",
    "    'module': 'gru'\n",
    "}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run the script with parallel processing\n",
    "    input_folder = \"try\"\n",
    "    output_folder = \"output\"\n",
    "    \n",
    "    # Use 75% of available CPU cores\n",
    "    num_processes = max(1,2)\n",
    "    \n",
    "    process_svc_folder_parallel(\n",
    "        input_folder,\n",
    "        output_folder,\n",
    "        timegan_params,\n",
    "        num_processes=num_processes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Github\\Thesis-Project\\timegan\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
